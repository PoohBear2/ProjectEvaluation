# Results/Suggestions/Observations
The model parameters used were in accordance to those used in the paper, in which I used 50 epochs, an initial learning rate of 0.001, and a decay factor of 0.9. My program was cloned from the github repo and into vscode for processing. The torch version used relied on CPU usage as to GPU (The Windows 2022 Surface Studio has an integrated gpu of the cpu), which consequently resulted in a ridiculously long processing time for one slide of 1 minute and 18 seconds. The device ran for over 3 days for anything to be produced, and this long processing time resulted in me having to cut out some of the preprocessed data. I reran the program after making an adjustment of 50 epochs to 20 epochs in the hopes of lessening the computational power and time needed, but the time for processing remained largely unchanged at 1 minute and 9 seconds. Compared to the result that were obtained from using the NVIDIA K40c graphics GPU card in the paper, which was 30 seconds, there is a stark contrast in efficiency. 

While I did not measure the time it took for the device to preprocess the slides, I can say from a qualitative standpoint that it took a little over a half hour to not only create the jpg files, but also to sort them in the correct directories. Having tested the same program on the Dell XPS 2019 model Windows laptop as well (which also lacks a gpu and actually crashed while running the train.py code for each of the 2 times that I tested it), I have found that Windows devices would require some sort of external gpu in order to process this deep learning module effectively. 
