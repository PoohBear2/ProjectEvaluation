# THE METRICS USED TO EVALUATE THE MODEL
The classification metrics used included accuracy, precision, recall, F1-score, and the area underneath the ROC Curve (AUC-ROC). These were all found within the evaluate_model() function within the utils_model.py program. I summarized each of the metrics below into a list:
1. Accuracy: In this case, the model achieved an accuracy of 0.8647, indicating that it correctly classified 86.47% of the test set. This metric is an indication of the proportion of correctly predicted samples out of the total number of samples available. 
2. Precision: This metric measures the ratio of true positive predictions to the total predicted positive samples that were present. The model with my dataset achieved a precision of 0.8222, indicating that 82.22% of the positive predictions were identified correctly.
3. Recall: This metric measures the proportion of true positive predictions out of the total actual positive samples. My program had a recall of 0.7260, which means that it captured 72.60% of the actual positive samples.
4. F1-score: This metric is a harmonic mean of both precision and recall. This provides a balanced evaluation of the model's performance that considers both positive and negative class cases. With an optimal number of 1, this model achieved 89.92%.
5. Area under the ROC Curve: Otherwise known as the AUC-ROC metric, this measures the area under the receiver operating characteristic (ROC) curve. The ROC curve is a graphical representation of the true positive rate (TPR) against the false positive rate (FPR) for different classification thresholds. The area achieved by the model was 0.9045, which is larger than 0.5, which indicates that the model is not just randomly guessing. With an optimal value of 1, this program was able to discriminate between the positive and negative classes relatively well.
